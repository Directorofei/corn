import os
import hashlib
from tqdm import tqdm
from collections import defaultdict

def calculate_md5(file_path, chunk_size=8192):
    """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                hash_md5.update(chunk)
    except IOError as e:
        print(f"Could not read file {file_path}: {e}")
        return None
    return hash_md5.hexdigest()

def get_category_from_path(file_path):
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–ç±»åˆ«ä¿¡æ¯"""
    path_parts = file_path.replace('\\', '/').split('/')
    if 'Healthy' in path_parts:
        return 'Healthy'
    elif 'Common_Rust' in path_parts:
        return 'Common_Rust'
    elif 'Blight' in path_parts:
        return 'Blight'
    elif 'Gray_Leaf_Spot' in path_parts:
        return 'Gray_Leaf_Spot'
    elif 'Blight_Rust' in path_parts:
        return 'Blight_Rust'
    elif 'Gray_Spot_Rust' in path_parts:
        return 'Gray_Spot_Rust'
    else:
        return 'Unknown'

def find_and_remove_duplicates(root_folder):
    """
    æŸ¥æ‰¾å¹¶åˆ é™¤æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„é‡å¤å›¾ç‰‡ã€‚
    æ”¯æŒæ–°çš„æ•°æ®é›†ç›®å½•ç»“æ„ï¼ŒåŒ…æ‹¬å¤åˆç—…ä¾‹ã€‚
    """
    hashes = defaultdict(list)
    files_to_scan = []
    category_stats = defaultdict(int)
    
    # å®šä¹‰é¢„æœŸçš„ç›®å½•ç»“æ„
    expected_dirs = [
        'Healthy',
        'Common_Rust',  # æ›´æ–°ç›®å½•å
        'Blight',
        'Gray_Leaf_Spot',
        'Compound_Cases/Blight_Rust',
        'Compound_Cases/Gray_Spot_Rust'
    ]
    
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†ç›®å½•ç»“æ„...")
    for expected_dir in expected_dirs:
        dir_path = os.path.join(root_folder, expected_dir)
        if os.path.exists(dir_path):
            print(f"  âœ… æ‰¾åˆ°: {expected_dir}")
        else:
            print(f"  âš ï¸  ç¼ºå¤±: {expected_dir}")
    
    # 1. æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„
    print("\nğŸ“‚ Step 1: æ‰«æå›¾ç‰‡æ–‡ä»¶...")
    valid_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}
    for subdir, _, files in os.walk(root_folder):
        for file in files:
            if os.path.splitext(file)[1].lower() in valid_extensions:
                file_path = os.path.join(subdir, file)
                files_to_scan.append(file_path)
                category = get_category_from_path(file_path)
                category_stats[category] += 1
    
    if not files_to_scan:
        print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ã€‚")
        return

    # æ˜¾ç¤ºæ‰«æç»Ÿè®¡
    print(f"\nğŸ“Š æ‰«æç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(files_to_scan)}")
    for category, count in category_stats.items():
        print(f"  {category}: {count} å¼ ")
    
    # ç‰¹åˆ«æé†’é”ˆç—…æ ·æœ¬æ•°é‡
    rust_count = category_stats.get('Common_Rust', 0)
    if rust_count < 500:
        print(f"\nâš ï¸  æ³¨æ„: é”ˆç—…æ ·æœ¬æ•°é‡è¾ƒå°‘({rust_count}å¼ )ï¼Œå»ºè®®è¡¥å……æ›´å¤šæ ·æœ¬")

    # 2. è®¡ç®—å“ˆå¸Œå€¼å¹¶æ‰¾å‡ºé‡å¤é¡¹
    print(f"\nğŸ” Step 2: è®¡ç®—å“ˆå¸Œå€¼å¹¶è¯†åˆ«é‡å¤é¡¹...")
    for filepath in tqdm(files_to_scan, desc="å¤„ç†å›¾ç‰‡"):
        file_hash = calculate_md5(filepath)
        if file_hash:
            hashes[file_hash].append(filepath)
            
    # 3. åˆ†æé‡å¤æƒ…å†µ
    print(f"\nğŸ§¹ Step 3: åˆ†æé‡å¤æƒ…å†µ...")
    duplicates_removed_count = 0
    category_duplicates = defaultdict(int)
    
    for file_hash, file_paths in tqdm(hashes.items(), desc="æ¸…ç†é‡å¤é¡¹"):
        if len(file_paths) > 1:
            # ä¿ç•™ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤å…¶ä½™çš„
            files_to_keep = file_paths[0]
            keep_category = get_category_from_path(files_to_keep)
            print(f"\n  ğŸ“Œ ä¿ç•™: {files_to_keep} [{keep_category}]")
            
            for duplicate_path in file_paths[1:]:
                dup_category = get_category_from_path(duplicate_path)
                print(f"    ğŸ—‘ï¸  åˆ é™¤: {duplicate_path} [{dup_category}]")
                category_duplicates[dup_category] += 1
                
                try:
                    os.remove(duplicate_path)
                    duplicates_removed_count += 1
                except OSError as e:
                    print(f"    âŒ åˆ é™¤å¤±è´¥: {duplicate_path}: {e}")
    
    # 4. æœ€ç»ˆç»Ÿè®¡
    print(f"\n" + "="*60)
    print(f"ğŸ‰ æ¸…ç†å®Œæˆ!")
    print(f"æ€»å¤„ç†å›¾ç‰‡: {len(files_to_scan)}")
    print(f"å”¯ä¸€å›¾ç‰‡: {len(hashes)}")
    print(f"åˆ é™¤é‡å¤é¡¹: {duplicates_removed_count}")
    
    print(f"\nğŸ“Š å„ç±»åˆ«åˆ é™¤ç»Ÿè®¡:")
    for category, count in category_duplicates.items():
        print(f"  {category}: åˆ é™¤äº† {count} å¼ é‡å¤å›¾ç‰‡")
    
    # æœ€ç»ˆå„ç±»åˆ«å‰©ä½™æ•°é‡
    print(f"\nğŸ“ˆ æ¸…ç†åå„ç±»åˆ«å‰©ä½™æ•°é‡:")
    final_stats = defaultdict(int)
    for file_hash, file_paths in hashes.items():
        if file_paths:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
            category = get_category_from_path(file_paths[0])
            final_stats[category] += 1
    
    for category, count in final_stats.items():
        print(f"  {category}: {count} å¼ ")
    
    print("="*60)

if __name__ == "__main__":
    dataset_directory = 'datasets'
    
    if not os.path.isdir(dataset_directory):
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ° '{dataset_directory}' ç›®å½•")
        print("è¯·ç¡®ä¿æ­¤è„šæœ¬ä¸ 'datasets' æ–‡ä»¶å¤¹åœ¨åŒä¸€ç›®å½•ä¸­")
    else:
        print("ğŸš¨ è­¦å‘Š: æ­¤è„šæœ¬å°†æ°¸ä¹…åˆ é™¤é‡å¤çš„å›¾ç‰‡æ–‡ä»¶")
        print(f"å®ƒå°†æ‰«æ '{dataset_directory}' ç›®å½•åŠå…¶å­ç›®å½•")
        print("å¯¹äºä»»ä½•ä¸€ç»„ç›¸åŒçš„å›¾ç‰‡ï¼Œå®ƒä¼šä¿ç•™ä¸€ä»½ï¼Œåˆ é™¤å…¶ä½™çš„")
        print("\næ”¯æŒçš„æ•°æ®é›†ç»“æ„:")
        print("  - Healthy/")
        print("  - Common_Rust/")
        print("  - Blight/")
        print("  - Gray_Leaf_Spot/")
        print("  - Compound_Cases/Blight_Rust/")
        print("  - Compound_Cases/Gray_Spot_Rust/")
        
        user_confirmation = input("\nç¡®å®šè¦ç»§ç»­å—? (yes/no): ")
        
        if user_confirmation.lower() == 'yes':
            print("\nğŸš€ å¼€å§‹å»é‡å¤„ç†...")
            find_and_remove_duplicates(dataset_directory)
        else:
            print("\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ") 