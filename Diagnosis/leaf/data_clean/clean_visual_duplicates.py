import os
from PIL import Image
import imagehash
from tqdm import tqdm
from collections import defaultdict
import sys

# æ£€æŸ¥ä¾èµ–é¡¹
try:
    from PIL import Image
    import imagehash
except ImportError:
    print("æ­¤è„šæœ¬éœ€è¦ 'Pillow' å’Œ 'imagehash' åº“")
    print("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…: pip install Pillow imagehash")
    sys.exit(1)

def get_category_from_path(file_path):
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–ç±»åˆ«ä¿¡æ¯"""
    path_parts = file_path.replace('\\', '/').split('/')
    if 'Healthy' in path_parts:
        return 'Healthy'
    elif 'Common_Rust' in path_parts:
        return 'Common_Rust'
    elif 'Blight' in path_parts:
        return 'Blight'
    elif 'Gray_Leaf_Spot' in path_parts:
        return 'Gray_Leaf_Spot'
    elif 'Blight_Rust' in path_parts:
        return 'Blight_Rust'
    elif 'Gray_Spot_Rust' in path_parts:
        return 'Gray_Spot_Rust'
    else:
        return 'Unknown'

def find_and_remove_visual_duplicates(root_folder, hash_size=8):
    """
    é€šè¿‡æ„ŸçŸ¥å“ˆå¸ŒæŸ¥æ‰¾å¹¶åˆ é™¤è§†è§‰ä¸Šé‡å¤çš„å›¾ç‰‡ã€‚
    æ”¯æŒæ–°çš„æ•°æ®é›†ç›®å½•ç»“æ„ï¼ŒåŒ…æ‹¬å¤åˆç—…ä¾‹ã€‚
    """
    hashes = defaultdict(list)
    files_to_scan = []
    category_stats = defaultdict(int)
    
    # å®šä¹‰é¢„æœŸçš„ç›®å½•ç»“æ„
    expected_dirs = [
        'Healthy',
        'Common_Rust',
        'Blight',
        'Gray_Leaf_Spot',
        'Compound_Cases/Blight_Rust',
        'Compound_Cases/Gray_Spot_Rust'
    ]
    
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†ç›®å½•ç»“æ„...")
    for expected_dir in expected_dirs:
        dir_path = os.path.join(root_folder, expected_dir)
        if os.path.exists(dir_path):
            print(f"  âœ… æ‰¾åˆ°: {expected_dir}")
        else:
            print(f"  âš ï¸  ç¼ºå¤±: {expected_dir}")
    
    # 1. æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„
    print("\nğŸ“‚ Step 1: æ‰«æå›¾ç‰‡æ–‡ä»¶...")
    valid_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}
    for subdir, _, files in os.walk(root_folder):
        for file in files:
            if os.path.splitext(file)[1].lower() in valid_extensions:
                file_path = os.path.join(subdir, file)
                files_to_scan.append(file_path)
                category = get_category_from_path(file_path)
                category_stats[category] += 1
    
    if not files_to_scan:
        print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ã€‚")
        return

    # æ˜¾ç¤ºæ‰«æç»Ÿè®¡
    print(f"\nğŸ“Š æ‰«æç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(files_to_scan)}")
    for category, count in category_stats.items():
        print(f"  {category}: {count} å¼ ")
    
    # ç‰¹åˆ«æé†’é”ˆç—…æ ·æœ¬æ•°é‡
    rust_count = category_stats.get('Common_Rust', 0)
    if rust_count < 500:
        print(f"\nâš ï¸  æ³¨æ„: é”ˆç—…æ ·æœ¬æ•°é‡è¾ƒå°‘({rust_count}å¼ )ï¼Œå»ºè®®è¡¥å……æ›´å¤šæ ·æœ¬")
        print("   è§†è§‰å»é‡å°†ç‰¹åˆ«å°å¿ƒå¤„ç†é”ˆç—…æ ·æœ¬")

    # 2. è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œå€¼å¹¶æ‰¾å‡ºé‡å¤é¡¹
    print(f"\nğŸ” Step 2: è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œå€¼ (hash_size={hash_size})...")
    for filepath in tqdm(files_to_scan, desc="å¤„ç†å›¾ç‰‡"):
        try:
            with Image.open(filepath) as img:
                # ä½¿ç”¨pHash (Perceptual Hash)
                file_hash = imagehash.phash(img, hash_size=hash_size)
                hashes[file_hash].append(filepath)
        except Exception as e:
            print(f"\nâŒ æ— æ³•å¤„ç†æ–‡ä»¶ {filepath}: {e}")
            
    # 3. åˆ é™¤è§†è§‰ä¸Šé‡å¤çš„æ–‡ä»¶
    print(f"\nğŸ§¹ Step 3: åˆ é™¤è§†è§‰é‡å¤æ–‡ä»¶...")
    duplicates_removed_count = 0
    category_duplicates = defaultdict(int)
    rust_preserved = 0
    
    for file_hash, file_paths in tqdm(hashes.items(), desc="æ¸…ç†é‡å¤é¡¹"):
        if len(file_paths) > 1:
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœåŒ…å«é”ˆç—…æ ·æœ¬ï¼Œä¼˜å…ˆä¿ç•™é”ˆç—…æ ·æœ¬
            rust_files = [f for f in file_paths if get_category_from_path(f) == 'Common_Rust']
            if rust_files and rust_count < 500:
                # é”ˆç—…æ ·æœ¬ä¸è¶³æ—¶ï¼Œä¼˜å…ˆä¿ç•™é”ˆç—…æ ·æœ¬
                files_to_keep = rust_files[0]
                rust_preserved += 1
                print(f"\n  ğŸ”¥ ä¼˜å…ˆä¿ç•™é”ˆç—…æ ·æœ¬: {files_to_keep}")
            else:
                # ä¿ç•™ç¬¬ä¸€ä¸ªæ–‡ä»¶
                files_to_keep = file_paths[0]
                keep_category = get_category_from_path(files_to_keep)
                print(f"\n  ğŸ“Œ ä¿ç•™: {files_to_keep} [{keep_category}]")
            
            for duplicate_path in file_paths[1:]:
                if duplicate_path != files_to_keep:
                    dup_category = get_category_from_path(duplicate_path)
                    print(f"    ğŸ—‘ï¸  åˆ é™¤ (è§†è§‰ç›¸ä¼¼): {duplicate_path} [{dup_category}]")
                    category_duplicates[dup_category] += 1
                    
                    try:
                        os.remove(duplicate_path)
                        duplicates_removed_count += 1
                    except OSError as e:
                        print(f"    âŒ åˆ é™¤å¤±è´¥: {duplicate_path}: {e}")
    
    # 4. æœ€ç»ˆç»Ÿè®¡
    print(f"\n" + "="*60)
    print(f"ğŸ‰ è§†è§‰å»é‡å®Œæˆ!")
    print(f"æ€»å¤„ç†å›¾ç‰‡: {len(files_to_scan)}")
    print(f"å”¯ä¸€å›¾ç‰‡ (è§†è§‰): {len(hashes)}")
    print(f"è§†è§‰é‡å¤é¡¹åˆ é™¤: {duplicates_removed_count}")
    
    if rust_preserved > 0:
        print(f"ğŸ”¥ é”ˆç—…æ ·æœ¬ä¿æŠ¤: {rust_preserved} ä¸ªé‡å¤ç»„ä¸­ä¼˜å…ˆä¿ç•™äº†é”ˆç—…æ ·æœ¬")
    
    print(f"\nğŸ“Š å„ç±»åˆ«åˆ é™¤ç»Ÿè®¡:")
    for category, count in category_duplicates.items():
        print(f"  {category}: åˆ é™¤äº† {count} å¼ è§†è§‰é‡å¤å›¾ç‰‡")
    
    # æœ€ç»ˆå„ç±»åˆ«å‰©ä½™æ•°é‡
    print(f"\nğŸ“ˆ æ¸…ç†åå„ç±»åˆ«å‰©ä½™æ•°é‡:")
    final_stats = defaultdict(int)
    for file_hash, file_paths in hashes.items():
        if file_paths:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
            category = get_category_from_path(file_paths[0])
            final_stats[category] += 1
    
    for category, count in final_stats.items():
        print(f"  {category}: {count} å¼ ")
    
    print("="*60)

if __name__ == "__main__":
    dataset_directory = 'datasets'
    
    if not os.path.isdir(dataset_directory):
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ° '{dataset_directory}' ç›®å½•")
        print("è¯·ç¡®ä¿æ­¤è„šæœ¬ä¸ 'datasets' æ–‡ä»¶å¤¹åœ¨åŒä¸€ç›®å½•ä¸­")
    else:
        print("ğŸ¯ --- è§†è§‰å»é‡è„šæœ¬ ---")
        print("æ­¤è„šæœ¬è¯†åˆ«å¹¶åˆ é™¤è§†è§‰ç›¸ä¼¼çš„å›¾ç‰‡ (å¦‚ç¿»è½¬ã€æ—‹è½¬)")
        print("å®ƒä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œ(pHash)æ¥æŸ¥æ‰¾è¿™äº›'å­ªç”Ÿ'å›¾ç‰‡")
        print("\næ”¯æŒçš„æ•°æ®é›†ç»“æ„:")
        print("  - Healthy/")
        print("  - Common_Rust/")
        print("  - Blight/")
        print("  - Gray_Leaf_Spot/")
        print("  - Compound_Cases/Blight_Rust/")
        print("  - Compound_Cases/Gray_Spot_Rust/")
        print("\nğŸš¨ è­¦å‘Š: æ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤æ–‡ä»¶")
        print("ğŸ”¥ ç‰¹åˆ«æ³¨æ„: é”ˆç—…æ ·æœ¬ä¸è¶³æ—¶å°†ä¼˜å…ˆä¿ç•™é”ˆç—…æ ·æœ¬")
        
        user_confirmation = input("\nç¡®å®šè¦ç»§ç»­å—? (yes/no): ")
        
        if user_confirmation.lower() == 'yes':
            print("\nğŸš€ å¼€å§‹è§†è§‰å»é‡å¤„ç†...")
            find_and_remove_visual_duplicates(dataset_directory, hash_size=8)
        else:
            print("\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ") 